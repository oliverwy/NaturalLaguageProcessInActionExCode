{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
    "from tensorflow.keras.layers import Conv1D,GlobalMaxPool1D\n",
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "def pre_process_data(filepath):\n",
    "    positive_path=os.path.join(filepath,'pos')\n",
    "    negative_path=os.path.join(filepath,'neg')\n",
    "    pos_label=1\n",
    "    neg_label=0\n",
    "    dataset=[]\n",
    "    for filename in glob.glob(os.path.join(positive_path,'*.txt')):\n",
    "        with open(filename,'r') as f:\n",
    "            dataset.append((pos_label,f.read()))\n",
    "    for filename in glob.glob(os.path.join(negative_path,'*.txt')):\n",
    "        with open(filename,'r') as f:\n",
    "            dataset.append((neg_label,f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pre_process_data('src/data/aclImdb/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Next to \"Star Wars\" and \"The Wizard of Oz,\" this remains one of the greatest fantasy films ever made. It\\'s a true shame it\\'s not as well-known as the former films (maybe because it sticks to a story based on legends rather than contemporary or sci-fi settings, and that it\\'s British, meaning a smaller market for films) but its wonderful to know that it\\'s deserved that reputation.<br /><br />Like all great family films, one can be a child, an adult, or even a teenager to enjoy this film (I\\'m currently 18), but one must appreciate classic films first. I absolutely adore this film. It has an extraordinary music score by Miklos Rozsa (perhaps my favorite classic film score) that rivals any John Williams \"Star Wars\" score, a fast but not flashy pace, beautiful sets, dialog, and use of color (both the sets and cinematography won Oscars), and state-of-the-art Oscar-winning special effects (for the time, and some are still stunning). And, of course, June Duprez\\'s sultry looks as the Princess rivals that of Catherine Zeta-Jones\\' (she even looks like Jones in a way!).<br /><br />In conclusion, this is one of my all-time favorite movie (next to \"The Adventures of Robin Hood\") and it truly deserves more attention. It is a true adventure of enchantment throughout, and, along with \"Robin Hood,\" it\\'s my desert island film that I could watch over and over again without getting annoyed.<br /><br />Stars: **** (excellent)'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<built-in method count of list object at 0x7fc2742c6a80>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# from nlpia.loaders import get_data\n",
    "word_vector=KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_and_vectorize(dataset):\n",
    "    tokenizer=TreebankWordTokenizer()\n",
    "    vectorized_data=[]\n",
    "    expected=[]\n",
    "    for sample in dataset:\n",
    "        tokens=tokenizer.tokenize(sample[1])\n",
    "        sample_vec=[]\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vec.append(word_vector[token])\n",
    "            except:\n",
    "                pass\n",
    "        expected.append(sample[0])\n",
    "        vectorized_data.append(sample_vec)\n",
    "    return vectorized_data,expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectored_dataset,expected=token_and_vectorize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(vectored_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_len(data,maxlen):\n",
    "    i=total_len=truncated=exact=padded=0\n",
    "\n",
    "    for sample in data:\n",
    "        i=i+1;\n",
    "        total_len+=len(sample)\n",
    "        if len(sample)>maxlen:\n",
    "            truncated+=1\n",
    "        elif len(sample)<maxlen:\n",
    "            padded+=1\n",
    "        else:\n",
    "            exact+=1\n",
    "        if i%1000==0:\n",
    "            print('处理了:{}'.format(i))\n",
    "    print(\"Padded:{}\".format(padded))\n",
    "    print(\"Equal:{}\".format(exact))\n",
    "    print(\"Truncated:{}\".format(truncated))\n",
    "    print('平均长度:{}'.format(total_len/len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "处理了:1000\n处理了:2000\n处理了:3000\n处理了:4000\n处理了:5000\n处理了:6000\n处理了:7000\n处理了:8000\n处理了:9000\n处理了:10000\n处理了:11000\n处理了:12000\n处理了:13000\n处理了:14000\n处理了:15000\n处理了:16000\n处理了:17000\n处理了:18000\n处理了:19000\n处理了:20000\n处理了:21000\n处理了:22000\n处理了:23000\n处理了:24000\n处理了:25000\nPadded:22458\nEqual:20\nTruncated:2522\n平均长度:205.2144\n"
     ]
    }
   ],
   "source": [
    "test_len(vectored_dataset,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(vectored_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point=int(len(vectored_dataset)*0.8)\n",
    "x_train=vectored_dataset[:split_point]\n",
    "y_train=expected[:split_point]\n",
    "x_test=vectored_dataset[split_point:]\n",
    "y_test=expected[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=400\n",
    "batch_size=32\n",
    "embedding_dims=300\n",
    "filtes=250\n",
    "kernel_size=3\n",
    "hidden_dims=250\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data,maxlen):\n",
    "    zero_vector=[]\n",
    "    new_data=[]\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    for sample in data:\n",
    "        if len(sample)>maxlen:\n",
    "            temp=sample[:maxlen]\n",
    "        elif len(sample)<maxlen:\n",
    "            temp=sample\n",
    "            additional_elems=maxlen-len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp=sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "x_train[0]\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x_train=pad_trunc(x_train,maxlen)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=pad_trunc(x_test,maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.reshape(x_train,(len(x_train),maxlen,embedding_dims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array(y_train)\n",
    "x_test=np.reshape(x_test,(len(x_test),maxlen,embedding_dims))\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv1D(filtes,kernel_size,padding='valid',activation='relu',strides=1,input_shape=(maxlen,embedding_dims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GlobalMaxPool1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "#输出层\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "#编译\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 34s 53ms/step - loss: 0.4868 - accuracy: 0.7452 - val_loss: 0.3246 - val_accuracy: 0.8632\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.2364 - accuracy: 0.9034 - val_loss: 0.3226 - val_accuracy: 0.8588\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 22s 36ms/step - loss: 0.1194 - accuracy: 0.9569 - val_loss: 0.3502 - val_accuracy: 0.8678\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.0447 - accuracy: 0.9861 - val_loss: 0.5252 - val_accuracy: 0.8464\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 23s 36ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.5014 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 22s 36ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: 0.4555 - val_accuracy: 0.8778\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 23s 36ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 0.7689 - val_accuracy: 0.8462\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 23s 36ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.5608 - val_accuracy: 0.8752\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 22s 36ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.5407 - val_accuracy: 0.8834\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.6553 - val_accuracy: 0.8712\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbadc317dc0>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "2021-02-20 16:13:16,836 WARNING:     tensorflow:333:            new_func From <ipython-input-27-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}