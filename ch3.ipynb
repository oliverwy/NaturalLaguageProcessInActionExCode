{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence=\"\"\"The faster Harry got to the store,the faster Harry,the faster,would get home.\"\"\"\n",
    "tokenizez=TreebankWordTokenizer()\n",
    "tokens=tokenizez.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words=Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "times_harry_appears=bag_of_words['harry']\n",
    "num_unique_words=len(bag_of_words)\n",
    "tf=times_harry_appears/num_unique_words\n",
    "round(tf,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizes=TreebankWordTokenizer()\n",
    "from nlpia.data.loaders import kite_text\n",
    "tokens=tokenizes.tokenize(kite_text.lower())\n",
    "tokens_counts=Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 26),\n",
       " ('a', 20),\n",
       " ('kite', 16),\n",
       " (',', 15),\n",
       " ('and', 10),\n",
       " ('of', 10),\n",
       " ('kites', 8),\n",
       " ('is', 7),\n",
       " ('in', 7),\n",
       " ('or', 6),\n",
       " ('wing', 5),\n",
       " ('to', 5),\n",
       " ('be', 5),\n",
       " ('as', 5),\n",
       " ('lift', 4),\n",
       " ('have', 4),\n",
       " ('may', 4),\n",
       " ('at', 3),\n",
       " ('so', 3),\n",
       " ('can', 3)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tokens_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopswords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[x for x in tokens if not x in stopswords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_counts=Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('kite', 16),\n",
       " (',', 15),\n",
       " ('kites', 8),\n",
       " ('wing', 5),\n",
       " ('lift', 4),\n",
       " ('may', 4),\n",
       " ('also', 3),\n",
       " ('kiting', 3),\n",
       " ('flown', 3),\n",
       " ('tethered', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "kite_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.07207207207207207,\n",
       " 0.06756756756756757,\n",
       " 0.036036036036036036,\n",
       " 0.02252252252252252,\n",
       " 0.018018018018018018,\n",
       " 0.018018018018018018,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "document_vector=[]\n",
    "dco_length=len(tokens)\n",
    "for key,value in kite_counts.most_common():\n",
    "    document_vector.append(value/dco_length)\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"The faster Harry go to the store, the faster and the faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "doc_tockens=[]\n",
    "for doc in docs:\n",
    "    doc_tockens+=[sorted(tokenizes.tokenize(doc.lower()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(doc_tockens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens=sum(doc_tockens,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'go',\n",
       " 'harry',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'store',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would',\n",
       " '.',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'than',\n",
       " '.',\n",
       " 'as',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon=sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'go',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('go', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "zero_vector=OrderedDict((token,0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[',', '.', 'and', 'as', 'faster', 'get', 'go', 'hairy', 'harry', 'home', 'is', 'jill', 'not', 'store', 'than', 'the', 'to', 'would']\n['the', 'faster', 'harry', 'go', 'to', 'the', 'store', ',', 'the', 'faster', 'and', 'the', 'faster', 'harry', 'would', 'get', 'home', '.']\nCounter({'the': 4, 'faster': 3, 'harry': 2, 'go': 1, 'to': 1, 'store': 1, ',': 1, 'and': 1, 'would': 1, 'get': 1, 'home': 1, '.': 1})\nOrderedDict([(',', 0.05555555555555555), ('.', 0.05555555555555555), ('and', 0.05555555555555555), ('as', 0), ('faster', 0.16666666666666666), ('get', 0.05555555555555555), ('go', 0.05555555555555555), ('hairy', 0), ('harry', 0.1111111111111111), ('home', 0.05555555555555555), ('is', 0), ('jill', 0), ('not', 0), ('store', 0.05555555555555555), ('than', 0), ('the', 0.2222222222222222), ('to', 0.05555555555555555), ('would', 0.05555555555555555)])\n['harry', 'is', 'hairy', 'and', 'faster', 'than', 'jill', '.']\nCounter({'harry': 1, 'is': 1, 'hairy': 1, 'and': 1, 'faster': 1, 'than': 1, 'jill': 1, '.': 1})\nOrderedDict([(',', 0), ('.', 0.05555555555555555), ('and', 0.05555555555555555), ('as', 0), ('faster', 0.05555555555555555), ('get', 0), ('go', 0), ('hairy', 0.05555555555555555), ('harry', 0.05555555555555555), ('home', 0), ('is', 0.05555555555555555), ('jill', 0.05555555555555555), ('not', 0), ('store', 0), ('than', 0.05555555555555555), ('the', 0), ('to', 0), ('would', 0)])\n['jill', 'is', 'not', 'as', 'hairy', 'as', 'harry', '.']\nCounter({'as': 2, 'jill': 1, 'is': 1, 'not': 1, 'hairy': 1, 'harry': 1, '.': 1})\nOrderedDict([(',', 0), ('.', 0.05555555555555555), ('and', 0), ('as', 0.1111111111111111), ('faster', 0), ('get', 0), ('go', 0), ('hairy', 0.05555555555555555), ('harry', 0.05555555555555555), ('home', 0), ('is', 0.05555555555555555), ('jill', 0.05555555555555555), ('not', 0.05555555555555555), ('store', 0), ('than', 0), ('the', 0), ('to', 0), ('would', 0)])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "doc_vector=[]\n",
    "print(lexicon)\n",
    "for doc in docs:\n",
    "    vec=copy.copy(zero_vector)\n",
    "    tokens=tokenizes.tokenize(doc.lower())\n",
    "    print(tokens)\n",
    "    token_counts=Counter(tokens)\n",
    "    print(token_counts)\n",
    "    for key,value in token_counts.items():\n",
    "        vec[key]=value/len(lexicon)\n",
    "    doc_vector.append(vec)\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('go', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.2222222222222222),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('go', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('go', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "brown.tagged_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs=set((',','.','--','-','!','?',':',';','``','(',')',\"''\",'[',']'))\n",
    "word_list=(x.lower() for x in brown.words() if x not in puncs)\n",
    "token_counts=Counter(word_list)\n",
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text,kite_history\n",
    "kite_intro=kite_text.lower()\n",
    "intro_token=tokenizes.tokenize(kite_intro)\n",
    "kite_history=kite_history.lower()\n",
    "history_token=tokenizes.tokenize(kite_history)\n",
    "intro_total=len(intro_token)\n",
    "intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "history_total=len(history_token)\n",
    "history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'kite': 0.0440771349862259}\nTerm Frequcency of 'Kite' in intro is:0.044077 \nTerm Frequcency of 'kite' in history is:0.020202\n"
     ]
    }
   ],
   "source": [
    "intro_tf={}\n",
    "history_tf={}\n",
    "intro_count=Counter(intro_token)\n",
    "# print(intro_count)\n",
    "intro_tf['kite']=intro_count['kite']/intro_total\n",
    "print(intro_tf)\n",
    "history_counts=Counter(history_token)\n",
    "history_tf['kite']=history_counts['kite']/history_total\n",
    "print(\"Term Frequcency of 'Kite' in intro is:{:4f} \".format(intro_tf['kite']))\n",
    "print(\"Term Frequcency of 'kite' in history is:{:4f}\".format(history_tf['kite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Term Frequcency of 'and' in intro is:0.027548 \nTerm Frequcency of 'and' in history is:0.030303\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and']=intro_count['and']/intro_total\n",
    "history_tf['and']=history_counts['and']/history_total\n",
    "print(\"Term Frequcency of 'and' in intro is:{:4f} \".format(intro_tf['and']))\n",
    "print(\"Term Frequcency of 'and' in history is:{:4f}\".format(history_tf['and']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_contains_and=0\n",
    "num_docs_contains_kite=0\n",
    "num_docs_contains_china=0\n",
    "for doc in [intro_token,history_token]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_contains_and+=1\n",
    "    if 'kite' in doc:\n",
    "        num_docs_contains_kite+=1\n",
    "    if 'china' in doc:\n",
    "        num_docs_contains_china+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['china']=intro_count['china']/intro_total\n",
    "history_tf['china']=history_counts['china']/history_total\n",
    "num_docs=2\n",
    "intro_idf={}\n",
    "history_idf={}\n",
    "intro_idf['and']=num_docs/num_docs_contains_and\n",
    "history_idf['and']=num_docs/num_docs_contains_and\n",
    "intro_idf['kite']=num_docs/num_docs_contains_kite\n",
    "history_idf['kite']=num_docs/num_docs_contains_kite\n",
    "intro_idf['china']=num_docs/num_docs_contains_china\n",
    "history_idf['china']=num_docs/num_docs_contains_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tfidf={}\n",
    "intro_tfidf['and']=intro_tf['and']/intro_idf['and']\n",
    "intro_tfidf['kite']=intro_tf['kite']/intro_idf['kite']\n",
    "intro_tfidf['china']=intro_tf['china']/intro_idf['china']\n",
    "history_tfidf={}\n",
    "history_tfidf['and']=history_tf['and']/history_idf['and']\n",
    "history_tfidf['kite']=history_tf['kite']/history_idf['kite']\n",
    "history_tfidf['china']=history_tf['china']/history_idf['china']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'and': 0.027548209366391185, 'kite': 0.0440771349862259, 'china': 0.0}\n{'and': 0.030303030303030304, 'kite': 0.020202020202020204, 'china': 0.005050505050505051}\n"
     ]
    }
   ],
   "source": [
    "print(intro_tfidf)\n",
    "print(history_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(intro_count['china'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'kite': 0.0440771349862259, 'and': 0.027548209366391185, 'china': 0.0}\n{'and': 1.0, 'kite': 1.0, 'china': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print(intro_tf)\n",
    "print(intro_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'kite': 0.020202020202020204, 'and': 0.030303030303030304, 'china': 0.010101010101010102}\n{'and': 1.0, 'kite': 1.0, 'china': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print(history_tf)\n",
    "print(history_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "history_counts['and']\n",
    "intro_count['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(intro_tf['china'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.010101010101010102\n"
     ]
    }
   ],
   "source": [
    "print(history_tf['china'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.005050505050505051"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "history_tfidf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tfidf_vectors=[]\n",
    "for doc in docs:\n",
    "    vec=copy.copy(zero_vector)\n",
    "    tokens=tokenizes.tokenize(doc.lower())\n",
    "    token_counts=Counter(tokens)\n",
    "    for key,value in token_counts.items():\n",
    "        docs_containing_key=0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key+=1\n",
    "        tf=value/len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf=len(docs)/docs_containing_key\n",
    "        else:\n",
    "            idf=0\n",
    "        vec[key]=tf*idf \n",
    "    doc_tfidf_vectors.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.25),\n",
       "              ('get', 0.16666666666666666),\n",
       "              ('go', 0.16666666666666666),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.16666666666666666),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.16666666666666666),\n",
       "              ('than', 0),\n",
       "              ('the', 0.6666666666666666),\n",
       "              ('to', 0.16666666666666666),\n",
       "              ('would', 0.16666666666666666)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.08333333333333333),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.08333333333333333),\n",
       "              ('get', 0),\n",
       "              ('go', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.16666666666666666),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('go', 0),\n",
       "              ('hairy', 0.08333333333333333),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('is', 0.08333333333333333),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.16666666666666666),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "doc_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The faster Harry go to the store, the faster and the faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('go', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6915981680357235\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def cosine_sim(vec1,vec2):\n",
    "    vec1=[val for val in vec1.values()]    \n",
    "    vec2=[val for val in vec2.values()]\n",
    "    dot_prod=0\n",
    "    \n",
    "    for i,v in enumerate(vec1):\n",
    "        dot_prod+=v*vec2[i]\n",
    "\n",
    "    mage_1=math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mage_2=math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod/(mage_2*mage_1)\n",
    "query=\"How long does it takes to get the store?\"\n",
    "query_vec=copy.copy(zero_vector)\n",
    "tokens=tokenizes.tokenize(query.lower())\n",
    "token_counts=Counter(tokens)\n",
    "for key,value in token_counts.items():\n",
    "    docs_containing_key=0\n",
    "    for _doc in docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key+=1\n",
    "    if docs_containing_key==0:\n",
    "        continue\n",
    "    tf=value/len(tokens)\n",
    "    idf=len(docs)/docs_containing_key\n",
    "    query_vec[key]=tf*idf\n",
    "print(cosine_sim(query_vec,doc_tfidf_vectors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(query_vec,doc_tfidf_vectors[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(query_vec,doc_tfidf_vectors[2]))\n"
   ]
  },
  {
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus=docs\n",
    "vec=TfidfVectorizer(min_df=1)\n",
    "model=vec.fit_transform(corpus)\n",
    "print(model.todense().round(2))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 59,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.14 0.   0.42 0.19 0.19 0.   0.22 0.19 0.   0.   0.   0.19 0.   0.74\n  0.19 0.19]\n [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n  0.   0.  ]\n [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n  0.   0.  ]]\n"
     ]
    }
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}