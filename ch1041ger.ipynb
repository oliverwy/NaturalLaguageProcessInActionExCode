{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d17217bd0d1dc75fc124b1baf47af913ae6650c56007a8cc26b22fb5e0065bb2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "chunk :10000\n",
      "chunk :10000\n",
      "chunk :10000\n",
      "chunk :10000\n",
      "chunk :10000\n",
      "chunk :10000\n",
      "chunk :4350\n",
      "加载处理完毕\n",
      "64350\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "input_texts,target_texts=[],[]\n",
    "input_vocabulary=set()\n",
    "output_vocabulary=set()\n",
    "start_token='\\t'\n",
    "stop_token='\\n'\n",
    "\n",
    "chunk_size=10000\n",
    "datalength=0\n",
    "max_encoder_seq_length=0\n",
    "max_decoder_seq_length=0\n",
    "\n",
    "filehandle=open('data/moviedialog.csv','r')\n",
    "df_iterator=pd.read_csv(filehandle,iterator=True,chunksize=chunk_size) #考虑到巨量文件的处理，本例子用不用不会产生任何影响，但是要是很巨大的文件，建议这么处理。\n",
    "loop=True\n",
    "while loop:\n",
    "    try:\n",
    "        chunk=df_iterator.get_chunk(chunk_size)\n",
    "        datalength+=len(chunk)\n",
    "        print(\"chunk :{}\".format(len(chunk)))\n",
    "        for input_t,target_t in zip(chunk.statement,chunk.reply):\n",
    "            for char in input_t:\n",
    "                if char not in input_vocabulary:\n",
    "                    input_vocabulary.add(char)\n",
    "                if char not in output_vocabulary:\n",
    "                    output_vocabulary.add(char)\n",
    "            max_encoder_seq_length=max_encoder_seq_length if max_encoder_seq_length>len(input_t) else len(input_t)\n",
    "            max_decoder_seq_length=max_decoder_seq_length if max_decoder_seq_length>len(start_token+target_t+stop_token) else len(start_token+target_t+stop_token)\n",
    "    except StopIteration:\n",
    "        loop=False\n",
    "        filehandle.seek(0)\n",
    "        df_iterator=pd.read_csv(filehandle,iterator=True,chunksize=chunk_size)\n",
    "        print(\"加载处理完毕\")\n",
    "output_vocabulary.add(start_token)\n",
    "output_vocabulary.add(stop_token)\n",
    "print(datalength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'l', 'b', '0', 'k', 'e', 'j', 'r', '6', '9', 'z', 'u', 'o', 'q', '?', '2', 'f', \"'\", 'w', '1', '8', 'c', 'x', 'h', 't', '.', ',', 'y', 'i', 'a', 'n', ';', 'd', '5', 'p', 's', 'g', ' ', ':', '!', '\\n', 'm', '4', '\\t', '7', '3', 'v'}\n{'l', 'b', '0', 'k', 'e', 'j', 'r', '6', '9', 'z', 'u', 'o', 'q', '?', '2', 'f', \"'\", 'w', '1', '8', 'c', 'x', 'h', 't', '.', ',', 'y', 'i', 'a', 'n', ';', 'd', '5', 'p', 's', 'g', ' ', ':', '!', 'm', '4', '7', '3', 'v'}\n100\n102\n"
     ]
    }
   ],
   "source": [
    "print(output_vocabulary)\n",
    "print(input_vocabulary)\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocabulary=sorted(input_vocabulary)\n",
    "output_vocabulary=sorted(output_vocabulary)\n",
    "input_vocab_size=len(input_vocabulary)\n",
    "output_vocab_size=len(output_vocabulary)\n",
    "input_token_index=dict([(char,i ) for i,char in enumerate(input_vocabulary)])\n",
    "target_token_index=dict([(char,i) for i,char in enumerate(output_vocabulary)])\n",
    "reverse_input_token_index=dict([(i,char) for i,char in enumerate(input_vocabulary)])\n",
    "reverse_target_token_index=dict([(i,char) for i,char in enumerate(output_vocabulary)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_iterator=pd.read_csv('data/moviedialog.csv',iterator=True,chunksize=chunk_size)\n",
    "def data_genertorfromcsv(datasource,batchsize,input_token_index,target_token_index,max_encoder_seq_length,max_decoder_seq_length):\n",
    "    encoder_input_datas=[]\n",
    "    decoder_input_datas=[]\n",
    "    decoder_target_datas=[]\n",
    "    filehandle=open(datasource,'r')\n",
    "    df_iterator=pd.read_csv(filehandle,iterator=True,chunksize=batchsize)\n",
    "    while True:\n",
    "        try:\n",
    "            chunk=df_iterator.get_chunk(batchsize)\n",
    "            for input_t,target_t in zip(chunk.statement,chunk.reply):\n",
    "                encoder_input_data=np.zeros((max_encoder_seq_length,len(input_token_index)))\n",
    "                decoder_input_data=np.zeros((max_decoder_seq_length,len(target_token_index)))\n",
    "                decoder_target_data=np.zeros((max_decoder_seq_length,len(target_token_index)))\n",
    "                for t,char in enumerate(input_t):\n",
    "                    encoder_input_data[t,input_token_index[char]]=1\n",
    "                target_t=start_token+target_t+stop_token\n",
    "                for yt,chart in enumerate(target_t):\n",
    "                    decoder_input_data[yt,target_token_index[chart]]=1\n",
    "                    if yt>0:\n",
    "                        decoder_target_data[yt-1,target_token_index[chart]]=1\n",
    "                \n",
    "                encoder_input_datas.append(encoder_input_data)\n",
    "                decoder_input_datas.append(decoder_input_data)\n",
    "                decoder_target_datas.append(decoder_target_data)\n",
    "            encoder_input_datas=np.reshape(encoder_input_datas,(len(encoder_input_datas),max_encoder_seq_length,len(input_token_index)))\n",
    "            decoder_input_datas=np.reshape(decoder_input_datas,(len(decoder_input_datas),max_decoder_seq_length,len(target_token_index)))\n",
    "            decoder_target_datas=np.reshape(decoder_target_datas,(len(decoder_target_datas),max_decoder_seq_length,len(target_token_index)))\n",
    "            yield [encoder_input_datas,decoder_input_datas],decoder_target_datas\n",
    "            encoder_input_datas=[]\n",
    "            decoder_input_datas=[]\n",
    "            decoder_target_datas=[]\n",
    "        except StopIteration:\n",
    "            filehandle.seek(0)\n",
    "            df_iterator=pd.read_csv(filehandle,iterator=True,chunksize=chunk_size)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM,Dense,Input\n",
    "batch_size=256\n",
    "epochs=100\n",
    "num_neurons=256\n",
    "encoder_inputs=Input(shape=(None,input_vocab_size))\n",
    "encoder=LSTM(num_neurons,return_state=True)\n",
    "encoder_outputs,state_h,state_c=encoder(encoder_inputs)\n",
    "encoder_states=[state_h,state_c]\n",
    "\n",
    "decoder_inputs=Input(shape=(None,output_vocab_size))\n",
    "decoder_lstm=LSTM(num_neurons,return_sequences=True,return_state=True)\n",
    "decoder_outputs,_,_=decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense=Dense(output_vocab_size,activation='softmax')\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics='acc')\n",
    "# model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=batch_size,epochs=epochs,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data_genertorfromcsv('data/moviedialog.csv',batchsize=batch_size,input_token_index=input_token_index,target_token_index=target_token_index,max_encoder_seq_length=max_encoder_seq_length,max_decoder_seq_length=max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "252/252 [==============================] - 10s 33ms/step - loss: 1.1146 - acc: 0.0592\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 1.0692 - acc: 0.0637\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 1.0456 - acc: 0.0677\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 1.0308 - acc: 0.0701\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 1.0089 - acc: 0.0746\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.9978 - acc: 0.0765\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 1.0476 - acc: 0.0649\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.9877 - acc: 0.0781\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.9723 - acc: 0.0814\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.9590 - acc: 0.0844\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.9423 - acc: 0.0878\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.9493 - acc: 0.0858\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.9273 - acc: 0.0912\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.9277 - acc: 0.0908\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.9305 - acc: 0.0892\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 8s 31ms/step - loss: 0.8728 - acc: 0.1024\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.8442 - acc: 0.1087\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.8255 - acc: 0.1130\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7956 - acc: 0.1197\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7729 - acc: 0.1249\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7545 - acc: 0.1297\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7389 - acc: 0.1337\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 8s 31ms/step - loss: 0.7275 - acc: 0.1369\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 8s 31ms/step - loss: 0.7206 - acc: 0.1390\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7088 - acc: 0.1422\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.7004 - acc: 0.1445\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6926 - acc: 0.1466\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6896 - acc: 0.1477\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6794 - acc: 0.1501\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6734 - acc: 0.1516\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6682 - acc: 0.1530\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6635 - acc: 0.1541\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6591 - acc: 0.1550\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6553 - acc: 0.1557\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6514 - acc: 0.1565\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6476 - acc: 0.1573\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6443 - acc: 0.1580\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6411 - acc: 0.1586\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6382 - acc: 0.1593\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.6354 - acc: 0.1606\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6328 - acc: 0.1612\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6303 - acc: 0.1616\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6280 - acc: 0.1621\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6258 - acc: 0.1626\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6242 - acc: 0.1629\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6224 - acc: 0.1633\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6202 - acc: 0.1637\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 8s 31ms/step - loss: 0.6183 - acc: 0.1641\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6163 - acc: 0.1646\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6146 - acc: 0.1649\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6128 - acc: 0.1653\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6112 - acc: 0.1656\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6095 - acc: 0.1659\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6082 - acc: 0.1662\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6065 - acc: 0.1665\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6052 - acc: 0.1666\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6039 - acc: 0.1667\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6024 - acc: 0.1670\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6013 - acc: 0.1672\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.6000 - acc: 0.1674\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5988 - acc: 0.1677\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5977 - acc: 0.1679\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5966 - acc: 0.1681\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5955 - acc: 0.1684\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5945 - acc: 0.1685\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5934 - acc: 0.1688\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5924 - acc: 0.1690\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5914 - acc: 0.1692\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5902 - acc: 0.1694\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5894 - acc: 0.1696\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5885 - acc: 0.1697\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5876 - acc: 0.1699\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5867 - acc: 0.1702\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5860 - acc: 0.1703\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5853 - acc: 0.1705\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5846 - acc: 0.1706\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5839 - acc: 0.1708\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5832 - acc: 0.1709\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5825 - acc: 0.1712\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5817 - acc: 0.1713\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5812 - acc: 0.1714\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5805 - acc: 0.1716\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5800 - acc: 0.1717\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5793 - acc: 0.1718\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5788 - acc: 0.1720\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5786 - acc: 0.1720\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5778 - acc: 0.1722\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5777 - acc: 0.1723\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5770 - acc: 0.1724\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5769 - acc: 0.1724\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5775 - acc: 0.1723\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5767 - acc: 0.1725\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5762 - acc: 0.1727\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5764 - acc: 0.1726\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.5780 - acc: 0.1721\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5756 - acc: 0.1729\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 8s 33ms/step - loss: 0.5745 - acc: 0.1731\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5736 - acc: 0.1734\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5731 - acc: 0.1735\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 8s 32ms/step - loss: 0.5726 - acc: 0.1736\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f52fbc38340>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "import math\n",
    "model.fit(d,epochs=epochs,steps_per_epoch=math.ceil(datalength/batch_size)) #利用ｍａｔｈ函数保证训练文本可以使用完，以免最后几条被丢弃，这里出去的时候没有创建验证数据生成器，格式是一样的，可以提前分好，加载的时候也是加载不同文件即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model=Model(encoder_inputs,encoder_states)\n",
    "thought_input=[Input(shape=(num_neurons,)),Input(shape=(num_neurons,))]\n",
    "decoder_outputs,state_h,state_c=decoder_lstm(decoder_inputs,initial_state=thought_input)\n",
    "decoder_states=[state_h,state_c]\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "decoder_model=Model(inputs=[decoder_inputs]+thought_input,outputs=[decoder_outputs]+decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sampple(preds,temperature=1.0):\n",
    "    preds=np.array(preds).astype('float64')\n",
    "    preds=np.log(preds)/temperature\n",
    "    exp_preds=np.exp(preds)\n",
    "    preds=exp_preds/np.sum(exp_preds)\n",
    "    probas=np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    thought=encoder_model.predict(input_seq)\n",
    "    target_seq=np.zeros((1,1,output_vocab_size))\n",
    "    target_seq[0,0,target_token_index[stop_token]]=1\n",
    "    stop_condition=False\n",
    "    generated_sequence=''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c=decoder_model.predict([target_seq]+thought)\n",
    "        # print(output_tokens[0,-1,:])\n",
    "        generated_token_idx=sampple(output_tokens[0,-1,:],temperature=0.3)#np.argmax(output_tokens[0,-1,:])\n",
    "        generated_char=reverse_target_token_index[generated_token_idx]\n",
    "        generated_sequence+=generated_char\n",
    "        if (generated_char==stop_token or len(generated_sequence)>max_decoder_seq_length):\n",
    "            stop_condition=True\n",
    "        target_seq=np.zeros((1,1,output_vocab_size))\n",
    "        target_seq[0,0,generated_token_idx]=1\n",
    "        thought=[h,c]\n",
    "    return generated_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(input_text):\n",
    "    input_seq=np.zeros((1,max_decoder_seq_length,input_vocab_size),dtype='float32')\n",
    "    for i ,char in enumerate(input_text):\n",
    "        input_seq[0,i,input_token_index[char]]=1\n",
    "    decode_sentence=decode_sequence(input_seq)\n",
    "    print('Bot reply:'+decode_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bot reply:what is it and the some to the care in the one and here to should that the care.\n\n"
     ]
    }
   ],
   "source": [
    "response(\"do you love girls?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}